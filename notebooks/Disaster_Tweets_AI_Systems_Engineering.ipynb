{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d77abc15",
   "metadata": {},
   "source": [
    "# Disaster Tweets — AI Systems Engineering (LLM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c6a9ce",
   "metadata": {},
   "source": [
    "A clear, GitHub‑ready notebook: **config → data → tokenize → train → evaluate → inference → submission → service**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58633e8c",
   "metadata": {},
   "source": [
    "## 1) Quickstart\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff916294",
   "metadata": {},
   "source": [
    "```bash\n",
    "pip install -e .\n",
    "ai-tweets train --config configs/default.yaml --train-csv data/train.csv\n",
    "ai-tweets serve --host 0.0.0.0 --port 8000\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09b8b1f",
   "metadata": {},
   "source": [
    "## 2) Config & Reproducibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952f8217",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json, random, numpy as np\n",
    "CFG = {\"seed\":42,\"model_name\":\"distilbert-base-uncased\",\"max_length\":256,\"epochs\":3,\"batch_size\":16,\n",
    "       \"learning_rate\":5e-5,\"weight_decay\":0.01,\"warmup_ratio\":0.1,\n",
    "       \"output_dir\":\"artifacts/model\",\"metrics_path\":\"artifacts/metrics.json\"}\n",
    "def set_seed(s=42):\n",
    "    random.seed(s); np.random.seed(s)\n",
    "    try:\n",
    "        import torch; torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
    "    except Exception: pass\n",
    "Path(CFG[\"output_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "Path(CFG[\"metrics_path\"]).parent.mkdir(parents=True, exist_ok=True)\n",
    "CFG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ece9cc",
   "metadata": {},
   "source": [
    "## 3) Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be880e7",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "CANDS_T = [Path('data/train.csv'), Path('./train.csv'), Path('../train.csv')]\n",
    "CANDS_E = [Path('data/test.csv'), Path('./test.csv'), Path('../test.csv')]\n",
    "first = lambda xs: next((p for p in xs if p.exists()), None)\n",
    "TRAIN_PATH, TEST_PATH = first(CANDS_T), first(CANDS_E)\n",
    "assert TRAIN_PATH is not None, 'Place train.csv under data/'\n",
    "df = pd.read_csv(TRAIN_PATH); text_col = 'text' if 'text' in df.columns else ('tweet' if 'tweet' in df.columns else None)\n",
    "assert text_col and 'target' in df.columns\n",
    "df[text_col] = df[text_col].astype(str); len(df), text_col\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f45b101",
   "metadata": {},
   "source": [
    "## 4) Tokenization & Dataset Prep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4479152c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "n_eval = max(1, int(len(df)*0.1)); df_tr, df_ev = df.iloc[:-n_eval].reset_index(drop=True), df.iloc[-n_eval:].reset_index(drop=True)\n",
    "train_ds, eval_ds = Dataset.from_pandas(df_tr[[text_col,'target']]), Dataset.from_pandas(df_ev[[text_col,'target']])\n",
    "tok = AutoTokenizer.from_pretrained(CFG['model_name'])\n",
    "tok_fn = lambda b: tok(b[text_col], truncation=True, max_length=CFG['max_length'])\n",
    "train_ds, eval_ds = train_ds.map(tok_fn, batched=True), eval_ds.map(tok_fn, batched=True)\n",
    "collator = DataCollatorWithPadding(tokenizer=tok); len(train_ds), len(eval_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b98bd5",
   "metadata": {},
   "source": [
    "## 5) Training (HF Trainer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17d2ddb",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "set_seed(CFG['seed'])\n",
    "args = TrainingArguments(output_dir=str(Path(CFG['output_dir'])), per_device_train_batch_size=CFG['batch_size'],\n",
    "    per_device_eval_batch_size=CFG['batch_size'], num_train_epochs=CFG['epochs'], learning_rate=CFG['learning_rate'],\n",
    "    weight_decay=CFG['weight_decay'], warmup_ratio=CFG['warmup_ratio'], evaluation_strategy='epoch', save_strategy='epoch',\n",
    "    logging_strategy='steps', logging_steps=50, load_best_model_at_end=True, metric_for_best_model='f1', greater_is_better=True, report_to=[])\n",
    "model = AutoModelForSequenceClassification.from_pretrained(CFG['model_name'], num_labels=2)\n",
    "metrics_fn = lambda ep: {'accuracy': accuracy_score(ep[1], ep[0].argmax(axis=-1)), 'f1': f1_score(ep[1], ep[0].argmax(axis=-1))}\n",
    "trainer = Trainer(model=model, args=args, train_dataset=train_ds, eval_dataset=eval_ds, tokenizer=tok, data_collator=collator, compute_metrics=metrics_fn)\n",
    "trainer.train(); metrics = trainer.evaluate()\n",
    "trainer.save_model(CFG['output_dir']); tok.save_pretrained(CFG['output_dir']); Path(CFG['metrics_path']).write_text(json.dumps(metrics, indent=2)); metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b951c01c",
   "metadata": {},
   "source": [
    "## 6) Evaluation (compact)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6966cd31",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = trainer.predict(eval_ds); y_true, y_pred = preds.label_ids, preds.predictions.argmax(axis=-1)\n",
    "from sklearn.metrics import classification_report\n",
    "print('Accuracy:', accuracy_score(y_true,y_pred)); print('F1      :', f1_score(y_true,y_pred))\n",
    "print('\\nClassification report:\\n', classification_report(y_true,y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5177a2a",
   "metadata": {},
   "source": [
    "## 7) Inference Demo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25cc8e4",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "clf = pipeline('text-classification', model=CFG['output_dir'], tokenizer=CFG['output_dir'])\n",
    "clf(['There is a huge fire downtown and people are evacuating!','Beautiful weather today, going to the park.'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04efa264",
   "metadata": {},
   "source": [
    "## 8) (Optional) Submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf7e572",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path, PurePath\n",
    "if TEST_PATH is not None:\n",
    "    df_te = pd.read_csv(TEST_PATH); assert text_col in df_te.columns\n",
    "    ys = clf(list(df_te[text_col].astype(str))); labels = [int(p['label'].replace('LABEL_','')) for p in ys]\n",
    "    sub = pd.DataFrame({'target': labels})\n",
    "    if 'id' in df_te.columns: sub.insert(0, 'id', df_te['id'].values)\n",
    "    outp = Path('artifacts')/'submission.csv'; outp.parent.mkdir(parents=True, exist_ok=True); sub.to_csv(outp, index=False); print('Saved ->', outp)\n",
    "else:\n",
    "    print('No test.csv detected — skipping submission')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7889a7cc",
   "metadata": {},
   "source": [
    "## 9) FastAPI Service (HTTP + WS + /metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30356860",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, WebSocket, WebSocketDisconnect, Response\n",
    "from pydantic import BaseModel\n",
    "from prometheus_client import Counter, Histogram, generate_latest, CONTENT_TYPE_LATEST\n",
    "import time, json\n",
    "from typing import Optional, List\n",
    "REQUEST_COUNT = Counter('inference_requests_total','', ['endpoint'])\n",
    "REQUEST_ERRORS = Counter('inference_errors_total','', ['endpoint'])\n",
    "LATENCY = Histogram('inference_latency_seconds','', ['endpoint'])\n",
    "class PredictIn(BaseModel):\n",
    "    text: Optional[str]=None; texts: Optional[List[str]]=None\n",
    "def make_app(model_dir: str):\n",
    "    from transformers import pipeline as hf_pipe\n",
    "    app = FastAPI(title='LLM Text Classification Service', version='1.0.0'); lp = hf_pipe('text-classification', model=model_dir, tokenizer=model_dir)\n",
    "    @app.get('/health')\n",
    "    def health(): return {'status':'ok'}\n",
    "    @app.post('/predict')\n",
    "    def predict(inp: PredictIn):\n",
    "        REQUEST_COUNT.labels(endpoint='/predict').inc(); t=time.perf_counter()\n",
    "        try:\n",
    "            batch = inp.texts if inp.texts else ([inp.text] if inp.text else None)\n",
    "            if not batch: raise ValueError('Provide \\'text\\' or \\'texts\\'')\n",
    "            outs = lp(batch, truncation=True)\n",
    "            norm = [{'label': o.get('label','LABEL_1').replace('LABEL_',''), 'score': float(o.get('score',0.0))} for o in outs]\n",
    "            return {'predictions': norm}\n",
    "        except Exception:\n",
    "            REQUEST_ERRORS.labels(endpoint='/predict').inc(); raise\n",
    "        finally:\n",
    "            LATENCY.labels(endpoint='/predict').observe(time.perf_counter()-t)\n",
    "    @app.get('/metrics')\n",
    "    def metrics(): return Response(content=generate_latest(), media_type=CONTENT_TYPE_LATEST)\n",
    "    @app.websocket('/ws')\n",
    "    async def ws(ws: WebSocket):\n",
    "        await ws.accept()\n",
    "        try:\n",
    "            while True:\n",
    "                msg = await ws.receive_text(); REQUEST_COUNT.labels(endpoint='/ws').inc(); t=time.perf_counter()\n",
    "                try:\n",
    "                    payload = json.loads(msg); text = payload.get('text')\n",
    "                    if not text:\n",
    "                        await ws.send_text(json.dumps({'error':'send JSON with \\'text\\''})); continue\n",
    "                    out = lp([text], truncation=True)[0]\n",
    "                    await ws.send_text(json.dumps({'label': out.get('label','LABEL_1').replace('LABEL_',''), 'score': float(out.get('score',0.0))}))\n",
    "                except Exception as e:\n",
    "                    REQUEST_ERRORS.labels(endpoint='/ws').inc(); await ws.send_text(json.dumps({'error': str(e)}))\n",
    "                finally:\n",
    "                    LATENCY.labels(endpoint='/ws').observe(time.perf_counter()-t)\n",
    "        except WebSocketDisconnect:\n",
    "            pass\n",
    "    return app\n",
    "app = make_app(CFG['output_dir']); print('Run: uvicorn notebook:app --reload --port 8000')\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
